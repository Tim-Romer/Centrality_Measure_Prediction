{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's your guidance on milestone 2. The big picture is as follows: now that you understand how to make networks and measure centrality, let's shift into predicting centrality using machine learning.\n",
    "\n",
    "The question is: given all the other centrality measures that I already have on a node, can I predict its centrality? E.g. given degree and betwenness centrality, can I predict closeness? Note that centralities are rankings. So you're only dealing with integers (rank 1, rank 2, rank 3...).\n",
    "Here is the methodology to answer this question:\n",
    "- for each type of network (random, SF, SM), create 100 instances equally split in networks of 100 nodes, 200 nodes, 400 nodes, and 800 nodes. (So you'd have 25 SF networks with 100 nodes for instance). In each instance, compute at least five different centrality measures for each node, and turn them into rankings. The result should go into a data.csv file, with headers such as\n",
    "\n",
    "|networkType|networkSize|instanceNumber|nodeNumber|degree rank|closeness rank|betweenness rank|...|\n",
    "|-----------|-----------|--------------|----------|-----------|--------------|-------------------|---|\n",
    "|scale-free | 100| 1| 1| 6| 2| 20| ...|\n",
    "|scale-free | 100| 1| 2| 5| 3| 21| ...|\n",
    "|scale-free | 100| 1| 4| 10| 4| 19| ...|\n",
    "    \n",
    "    \n",
    "(The above shows three nodes from one instance of a scale-free graph of size 100)\n",
    "\n",
    "- compute the pairwise correlations between the rankings across all network types, as well as within each network type (random, SF, SM). We want this as a 'baseline'. The idea is that we should be able to make accurate predictions even where there didn't appear to be a linear correlation.\n",
    "- we'll use machine learning algorithms for predictions. The logic is that we first try to make coarse predictions. If it doesn't work (=inaccurate) then we get even more coarse. If it works, we can try to be more precise. In other words, we don't start shooting for the moon but we try to find a middle ground (think of it as a binary search). Our starting point will be to predict whether one centrality of a node is in the top 25% based on the other centrality. For instance, given the ranks in degree, closeness, etc., is the node's betweenness in the top 25% or not (yes/no)? This is a binary classification. The steps would be to create the class outcome columns (data preparation). Then, for each prediction, ensure that the original centrality is removed (e.g. if you predict top 25% degree then you should not be using degree rank!), balance the data (so we have 50-50 of yes/no), and predict. I recommend using at least three different algorithms (e.g., decision trees, support vector machine, random forests; see textbook chapters on the last two) and a ten-fold cross-validation (also shown in the textbook). Balancing and cross-validation will be discussed in the upcoming Tuesday class. We'd like results (=accuracy) to be presented overall as well as divided per network type (so that we can see e.g. if we're better at making predictions in small-world than in random networks).\n",
    "\n",
    "This milestone also comes with a bonus of 10% of your milestone 2 grade. That is, if you achieve the bonus in its entirety, whatever grade you got will be multiplied by 1.10. The bonus consists of doing all of the above on the forth type of network: small-world scale-free. Which means you need to be able to demonstrate that you can generate such networks for starters (fit of degree distribution / low average path length / high clustering). You can use google scholar to find a model that generates such networks, you do not need to invent them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Generation\n",
    "\n",
    "The five centralities that we will examine first are degree centrality, closeness centrality, betweeness centrality, load flow centrality, and reaching centrality. All of the data for all of the graphs will be loaded into centrality_data.csv. A copy of the data will be put into data.csv just in case centrality_data.csv gets over-written. All in all, the code takes about an hour to run on Tim's machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degrees(G):\n",
    "    deg = nx.degree_centrality(G)\n",
    "    deg_df = pd.Series(deg).to_frame()\n",
    "    deg_df.columns = ['Degree']\n",
    "    deg_df['degree_rank'] = deg_df['Degree'].rank(method = 'min', ascending = False)\n",
    "    return deg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closeness(G):\n",
    "    deg = nx.degree_centrality(G)\n",
    "    deg_df = pd.Series(deg).to_frame()\n",
    "    deg_df.columns = ['Closeness']\n",
    "    deg_df['closeness_rank'] = deg_df['Closeness'].rank(method = 'min', ascending = False)\n",
    "    return deg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betweeness(G):\n",
    "    deg = nx.betweenness_centrality(G)\n",
    "    deg_df = pd.Series(deg).to_frame()\n",
    "    deg_df.columns = ['Betweeness']\n",
    "    deg_df['betweeness_rank'] = deg_df['Betweeness'].rank(method = 'min', ascending = False)\n",
    "    return deg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_flow(G):\n",
    "    deg = nx.load_centrality(G)\n",
    "    deg_df = pd.Series(deg).to_frame()\n",
    "    deg_df.columns = ['Load']\n",
    "    deg_df['load_rank'] = deg_df['Load'].rank(method = 'min', ascending = False)\n",
    "    return deg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reaching(G):\n",
    "    deg = nx.load_centrality(G)\n",
    "    deg_df = pd.Series(deg).to_frame()\n",
    "    deg_df.columns = ['Reaching']\n",
    "    deg_df['reach_rank'] = deg_df['Reaching'].rank(method = 'min', ascending = False)\n",
    "    return deg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL. IT WILL OVERWRITE DATA AND TAKE AN HOUR TO RUN.\n",
    "with open('centrality_data.csv', 'w') as wr:\n",
    "    # Write the column headers to csv\n",
    "    wr.write('networkType, networkSize, instanceNumber, nodeNumber, degreeRank, closenessRank, betweennessRank, loadRank, reachRank\\n')\n",
    "    networkTypes = ['scale-free', 'small-world', 'random']\n",
    "    sizes = [100, 200, 400, 800]\n",
    "    G = nx.scale_free_graph(100)\n",
    "    \n",
    "    for netType in networkTypes:\n",
    "        for size in sizes:\n",
    "            for instNum in range(1, 100):\n",
    "                if netType == 'scale-free':\n",
    "                    G = nx.scale_free_graph(size)\n",
    "                elif netType == 'small-world':\n",
    "                    G = nx.watts_strogatz_graph(size, 3, 0.5)\n",
    "                else:\n",
    "                    G = nx.gnm_random_graph(size, size * 4)\n",
    "                degree_list = get_degrees(G)\n",
    "                closeness_list = get_closeness(G)\n",
    "                betweeness_list = get_betweeness(G)\n",
    "                load_list = get_current_flow(G)\n",
    "                reach_list = get_global_reaching(G)\n",
    "                for node in G.nodes():\n",
    "                    wr.write(netType + ', ' + str(size)+ ', ' + str(instNum) + ', ')\n",
    "                    wr.write(str(node) + ', ' + str(degree_list['degree_rank'][node]) + ', ')\n",
    "                    wr.write(str(closeness_list['closeness_rank'][node]) + ', ')\n",
    "                    wr.write(str(betweeness_list['betweeness_rank'][node]) + ', ')\n",
    "                    wr.write(str(load_list['load_rank'][node]) + ', ')\n",
    "                    wr.write(str(reach_list['reach_rank'][node]) + '\\n')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
